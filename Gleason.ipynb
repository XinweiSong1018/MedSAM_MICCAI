{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2dee088f-8025-4360-92be-30820f4f626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Hu.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b2596992-17ea-4de6-9864-e3c77fc25600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gleason(Dataset):\n",
    "    def __init__(self, imgdir, maskdir=None, train=True, val=False,\n",
    "                 test=False, transforms=None, transform=None, target_transform=None):\n",
    "        super(Gleason, self).__init__()\n",
    "        self.imgdir = imgdir\n",
    "        self.maskdir = maskdir\n",
    "        self.imglist = sorted(os.listdir(imgdir))[1:]\n",
    "        if not test:\n",
    "            self.masklist = [item.replace('.jpg', '_classimg_nonconvex.png') for item in self.imglist]\n",
    "        else:\n",
    "            self.masklist = []\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.transforms = transforms\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imglist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(osp.join(self.imgdir, self.imglist[idx]))\n",
    "        if not self.test:\n",
    "            mask = Image.open(osp.join(self.maskdir, self.masklist[idx]))\n",
    "            y_indices, x_indices = np.where(np.array(mask) > 0)\n",
    "            if x_indices.size > 0 and y_indices.size > 0:\n",
    "                x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "                y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "                H, W = np.array(mask).shape\n",
    "                x_min = max(0, x_min - random.randint(0, 60))\n",
    "                x_max = min(W, x_max + random.randint(0, 60))\n",
    "                y_min = max(0, y_min - random.randint(0, 60))\n",
    "                y_max = min(H, y_max + random.randint(0, 60))\n",
    "                box = np.array([x_min, y_min, x_max, y_max])\n",
    "            else:\n",
    "                box = np.array([0, 0, 0,0])\n",
    "        if self.transforms and not self.test:\n",
    "            image, mask, box = self.transforms(image, mask, box)\n",
    "        if self.transform:\n",
    "            image = self.transform(image, mask)\n",
    "        if self.target_transform and not self.test:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        if self.test:\n",
    "            return image\n",
    "        else:\n",
    "            return image, mask, box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fea516d3-660e-495a-bdd9-078930c1de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(imgdir, maskdir=None, train=True, val=False, test=False,\n",
    "                transforms=None, transform=None, target_transform=None):\n",
    "    dataset = Gleason(imgdir=imgdir, maskdir=maskdir, train=train,\n",
    "                      val=val, test=test, transforms=transforms,\n",
    "                      transform=transform, target_transform=target_transform)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    base_size = 1000\n",
    "    crop_size = 768\n",
    "\n",
    "    min_size = int((0.5 if train else 1.0) * base_size)\n",
    "    max_size = int((2.0 if train else 1.0) * base_size)\n",
    "    transforms = []\n",
    "    transforms.append(T.RandomResize(min_size, max_size))\n",
    "    if train:\n",
    "        transforms.append(T.ColorJitter(0.5, 0.5, 0.5, 0.5))\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(0.5))\n",
    "        transforms.append(T.RandomCrop(crop_size))\n",
    "    transforms.append(ToTensor())\n",
    "    transforms.append(Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225]))\n",
    "\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "81e6bb7b-fd33-4cb5-b003-75c2b22eeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(imgdir='../data/TrainImgs',\n",
    "                          maskdir='../data/labels',\n",
    "                          train=True,\n",
    "                          val=False,\n",
    "                          test=False,\n",
    "                          transforms=get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ee28c5a4-1e84-4f72-af45-64dddfc1febf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RandomResize.__call__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image, mask, box \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[208], line 39\u001b[0m, in \u001b[0;36mGleason.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         box \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest:\n\u001b[0;32m---> 39\u001b[0m     image, mask, box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     41\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image, mask)\n",
      "Cell \u001b[0;32mIn[198], line 29\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, image, target, box)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, target, box):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 29\u001b[0m         image, target, box \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target, box\n",
      "\u001b[0;31mTypeError\u001b[0m: RandomResize.__call__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "image, mask, box = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7d5f23a0-bb5b-48f0-b9b5-70c01e1c09de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768, 768])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ef126535-c4e9-4782-976d-30c5734c700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.0000,   0.0000, 756.4580, 768.0000])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8ef63-ce09-4f94-b55b-697bed5b0431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
